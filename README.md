1.	Install the following to your python interruptor: pandas, matplotlib, seaborn, scikit-learn, numpy, and scrapy.
2.	Run the scrapy spider in the file called “matches.py” using scrapy commands. It will write all the data to a csv file called “games.csv”. The data is already in there but if you choose to run it again, it will take around 10-15 minutes to write everything to the file. If you get a 403 error when scrapping the websites, make sure that “COOKIES_ENABLED” is set to false. The settings for the scrapy spider is found in the ‘settings.py’ file.
3.	Once all the data is written to the csv file, there should be a total of 2387 items (could be more if the website is updated with more games). Run the “games_visualized.py” file. There are three functions commented out in main(), they all produce a visual. “table_foul_stats(df)” shows foul stats. NOTE, there are five more commented out functions in “table_foul_stats(df)” function at the top of the file. You have to uncomment one in the function as well. Can only uncomment one at a time. “table_result_stats(df)” shows result stats. “model(df)” is the machine learning function. This function will also produce a visual, it takes about 5 seconds to appear. There is also information output in the terminal. Only one function can be uncommented at a time in main()!
